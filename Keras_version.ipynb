{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/moliere_1_clean.txt\",encoding='utf-8') as f1:\n",
    "    moliere_1 = f1.read()\n",
    "with open(\"data/moliere_2_clean.txt\",encoding='utf-8') as f2:\n",
    "    moliere_2 = f2.read()\n",
    "with open(\"data/moliere_3_clean.txt\",encoding='utf-8') as f3:\n",
    "    moliere_3 = f3.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1275253"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = moliere_1 + ' ' + moliere_2 + ' ' + moliere_3\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eh bien, Sabine, quel conseil me donnes-tu? Vraiment, il y a bien des nouvelles. Mon oncle veut réso'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace('œ','oe')\n",
    "text = text.replace('æ','ae')\n",
    "text = text.replace('î','i')\n",
    "text = text.replace('ï','i')\n",
    "text = text.replace('º','')\n",
    "text = text.replace('_','')\n",
    "text = text.replace('ñ','n')\n",
    "text = text.replace('λ','')\n",
    "text = text.replace('ο','')\n",
    "text = text.replace('ρ','')\n",
    "text = text.replace('ς','')\n",
    "text = text.replace('φ','')\n",
    "text = text.replace('β','')\n",
    "text = text.replace('ε','')\n",
    "text = text.replace('ι','')\n",
    "\n",
    "text = text.replace('É','E')\n",
    "text = text.replace('È','E')\n",
    "text = text.replace('Ê','E')\n",
    "text = text.replace('Ç','C')\n",
    "\n",
    "text = text.replace('\\n',' ')\n",
    "\n",
    "text = re.sub(r'( )+',' ',text) #remove multiple spaces\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 86\n",
      "[' ', '!', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '«', '»', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'ô', 'ù', 'û']\n"
     ]
    }
   ],
   "source": [
    "character_list = sorted(set(text))\n",
    "N_char = len(character_list)\n",
    "\n",
    "print('Length of vocab:',N_char)\n",
    "print(character_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(sent) for sent in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, ';': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'X': 43, 'Y': 44, 'Z': 45, '[': 46, ']': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, '«': 74, '»': 75, 'à': 76, 'â': 77, 'ç': 78, 'è': 79, 'é': 80, 'ê': 81, 'ë': 82, 'ô': 83, 'ù': 84, 'û': 85}\n"
     ]
    }
   ],
   "source": [
    "char2code = {}\n",
    "for k, word in enumerate(character_list):\n",
    "    char2code[word] = k\n",
    "    \n",
    "print(char2code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '!', 2: \"'\", 3: '(', 4: ')', 5: ',', 6: '-', 7: '.', 8: '0', 9: '1', 10: '2', 11: '3', 12: '4', 13: '5', 14: '6', 15: '7', 16: '8', 17: '9', 18: ':', 19: ';', 20: '?', 21: 'A', 22: 'B', 23: 'C', 24: 'D', 25: 'E', 26: 'F', 27: 'G', 28: 'H', 29: 'I', 30: 'J', 31: 'K', 32: 'L', 33: 'M', 34: 'N', 35: 'O', 36: 'P', 37: 'Q', 38: 'R', 39: 'S', 40: 'T', 41: 'U', 42: 'V', 43: 'X', 44: 'Y', 45: 'Z', 46: '[', 47: ']', 48: 'a', 49: 'b', 50: 'c', 51: 'd', 52: 'e', 53: 'f', 54: 'g', 55: 'h', 56: 'i', 57: 'j', 58: 'k', 59: 'l', 60: 'm', 61: 'n', 62: 'o', 63: 'p', 64: 'q', 65: 'r', 66: 's', 67: 't', 68: 'u', 69: 'v', 70: 'w', 71: 'x', 72: 'y', 73: 'z', 74: '«', 75: '»', 76: 'à', 77: 'â', 78: 'ç', 79: 'è', 80: 'é', 81: 'ê', 82: 'ë', 83: 'ô', 84: 'ù', 85: 'û'}\n"
     ]
    }
   ],
   "source": [
    "code2char = {v:k for k,v in char2code.items()}\n",
    "print(code2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#categorical_labels = to_categorical(int_labels, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hots(sequence, vocab_size=N_char):\n",
    "    result = np.zeros((len(sequence), vocab_size))\n",
    "    for k,s in enumerate(sequence):\n",
    "        idx = char2code[s]\n",
    "        result[k, idx] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textify(embedding,character_list = character_list):\n",
    "    result = \"\"\n",
    "    indices = np.argmax(embedding, axis=1)\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1274945, 86)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = one_hots(text)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 64\n",
    "step = 16\n",
    "L = raw.shape[0]\n",
    "\n",
    "x,y = [], []\n",
    "for k in np.arange(window_size,L,step):\n",
    "    #print(k,k+window_size)\n",
    "    x.append(raw[k-window_size:k,:])  #Up until (but excluding) k\n",
    "    y.append(raw[k,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79681, 64, 86) (79681, 86)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(x)\n",
    "Y = np.array(y)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up\n",
    "del x, y, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55776, 64, 86) (55776, 86) (23905, 64, 86) (23905, 86)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y,train_size=0.7,shuffle=False)\n",
    "print(Xtrain.shape,Ytrain.shape,Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79681"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_samples = X.shape[0]\n",
    "N_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Dense, GRU, Embedding, Dropout, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, BaseLogger\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False)\n",
    "\n",
    "Input shape: 3D tensor with shape (batch_size, timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(N_char,256))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(GRU(256, recurrent_dropout=0.25))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.add(Dense(N_char,activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.05,decay=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['acc'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               351232    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 86)                22102     \n",
      "=================================================================\n",
      "Total params: 373,334\n",
      "Trainable params: 373,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(window_size, N_char),recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(N_char, activation='softmax'))\n",
    "\n",
    "adam_optimizer = Adam(lr=0.01, decay=1e-4)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint\n",
    "filepath=\"weights-keras.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#logger = BaseLogger(stateful_metrics=['loss','val_loss','acc','val_acc'])\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fit(model,X,Y,**kwargs):\n",
    "    \n",
    "    # Train model and output new history\n",
    "    new_hist = model.fit(X,Y,**kwargs)\n",
    "    \n",
    "    # Update new history with data from the old one\n",
    "    try:\n",
    "        # Update metrics\n",
    "        for k,v in history.history.items():\n",
    "            hist.history[k] = history.history[k] + new_hist.history[k]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Update epochs\n",
    "    new_hist.epoch = list(range(1,len(new_hist.history['acc'])))\n",
    "    \n",
    "    return new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55776 samples, validate on 23905 samples\n",
      "Epoch 1/10\n",
      "55776/55776 [==============================] - 190s 3ms/step - loss: 1.8194 - acc: 0.4547 - val_loss: 1.7794 - val_acc: 0.4759\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.48078\n",
      "Epoch 2/10\n",
      "55776/55776 [==============================] - 189s 3ms/step - loss: 1.7973 - acc: 0.4607 - val_loss: 1.7620 - val_acc: 0.4796\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.48078\n",
      "Epoch 3/10\n",
      "43008/55776 [======================>.......] - ETA: 44s - loss: 1.7701 - acc: 0.4680"
     ]
    }
   ],
   "source": [
    "#history = model.fit(Xtrain, Ytrain, batch_size=256, epochs=3, validation_data=(Xtest,Ytest),callbacks=callbacks_list)\n",
    "history = my_fit(model,Xtrain,Ytrain, batch_size=256, epochs=10, validation_data=(Xtest,Ytest),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "with open('keras_log - {:%Y-%m-%d %H_%M}.txt'.format(now),'w') as file:\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    for e,l,vl,a,va in zip(history.epoch,loss,val_loss,acc,val_acc):\n",
    "        file.write('Epoch {:03d} | Train_loss {:.3f} | Val_loss {:.3f} | Train_acc {:.3f} | Val_acc {:.3f}\\n'.format(e,l,vl,a,va));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(prefix):\n",
    "    L = len(prefix)\n",
    "    temp = np.zeros(shape=(window_size,N_char))\n",
    "    temp[-L:,:] = one_hots(prefix)[-window_size:]\n",
    "    return model.predict(temp.reshape(1,-1,N_char)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    #preds = np.log(preds) / temperature\n",
    "    #exp_preds = np.exp(preds)\n",
    "    exp_preds = preds * np.exp(-temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_sample(prefix,n=10,temperature=1.0):\n",
    "    result = []\n",
    "    for _ in range(n):\n",
    "        this = sample(predict_next(prefix),temperature=temperature)\n",
    "        this = code2char[this]\n",
    "        result.append(this)\n",
    "        prefix+=this\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_following(prefix,n=20,temperature=1.0):\n",
    "    return prefix + ''.join(multi_sample(prefix,n,temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.05,0.1,0.2,0.5,1,2]:\n",
    "    print('Temp:',temp,'\\t',predict_following('Bonjour cher ami, que je suis heureux de vo',temperature=temp,n=60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.figure(figsize=(24, 8), dpi= 600);\n",
    "fig, axes = plt.subplots(2,sharex=True,figsize=(16, 6));\n",
    "ax1,ax2 = axes\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "ax1.plot(history.history['loss'],'b.-',label='train_loss')\n",
    "ax1.plot(history.history['val_loss'],'r.-',label='validation_loss')\n",
    "ax1.grid();\n",
    "ax1.legend();\n",
    "\n",
    "ax2.plot(history.history['acc'],'cx-',label='train_acc');\n",
    "ax2.plot(history.history['val_acc'],'mx-',label='validation_acc');\n",
    "ax2.grid();\n",
    "ax2.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
